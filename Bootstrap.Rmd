---
title: "Managing Model Uncertainty<BR>
Part 2: Bootstrap"
author: "Vadim Bondarenko<BR>FI Consulting"
date: "July 15, 2015"
output: 
  html_document: 
    highlight: haddock
    number_sections: yes
    theme: flatly
    toc: yes
---

<style>
caption{color:black;font-size=16px;font-weight: bold;}
</style>


```{r setup, include=FALSE}

library(knitr)
library(pander)
library(ggplot2)
library(ggthemes)
library(scales)
library(png)
library(grid)
library(car)
library(boot)
library(ROCR)
library(dplyr)

# options("scipen"=2)
opts_chunk$set(echo = TRUE, results='asis', fig.align = 'center')
theme_set(theme_classic())
panderOptions('table.split.table', Inf)
panderOptions("big.mark", ",")

```


```{r logo, echo=FALSE, fig.width=600/72*1.2, fig.height=147/72*1.2}
# logo
img <- readPNG('img/webinar_logo.png')
grid.raster(img)
```


# What is Bootstrap?


## Basic Example: Confidence Intervals of a Sample Median  

### The Data
Suppose we have a sample __x__ of size __n=9__, which is simply numbers 1 through 9. We assume that __x__ is a representative sample of independent observation from a larger unknown population __X__.

```{r}
# hypothetical sample x = {1,2, ... ,8,9}
x = seq(1:9)

# print x
matrix(x, nrow = 1, dimnames = list("x", paste('Obs', 1:9))) %>% 
    pandoc.table("Sample Observations x")

# print summary of x
summary(x) %>% t() %>%  pandoc.table("Summary of sample x")
```


The observe __median = `r median(x)`__ (the middle __Obs 5__), but if __x__ is a representative sample from a population and __`r median(x)`__ is the estimate of median for that population, what is the __90% confidence interval__ around that estimate?

There is no analytical way to derive the distribution of the median. However, we can use bootstrap to derive it empirically from the data.

### Basic Bootrap  
To illustrate the basic setup, we begin by randomly drawing **R=10** samples __with replacement__ from out  sample __x__. Each random draw is of the same length (n=9) as our original sample.

```{r}

# set up a empty matrix to populate Bootstrap draws
B = matrix(nrow = 10, ncol = 9,
           dimnames = list(paste('Bootstrap Sample',1:10), 
                           LETTERS[1:9]))
# loop 10 times
set.seed(111)
for(i in 1:10){
    # draw random samples from x
    B[i,] = sample(x, size = length(x), replace = TRUE) %>% sort()
}


```

As a result, we have a __R x n__ matrix of bootstrapped observations.  
```{r echo=FALSE}

matrix(x, nrow = 1,ncol = 9,
       dimnames = list("Original Sample x", 
                       paste("Obs", 1:9))) %>% 
    rbind(B) %>% 
    pandoc.table(emphasize.strong.cols=5,
                 emphasize.strong.rows=1, caption='Bootstrap Results')

```


Since we drew 10 random samples of the same size as our original sample (9), any given observation **x<sub>i</sub>** will appear more than once in some samples and none at all in others. For example, the number 4 appears four times in Sample 1, but is missing completely from bootstrap Sample 2.

The middle column with __Observation 5__ contains the medians of each bootstrapped sample. It provides us with the empirical distribution of the median, from which we can infer the mean, quantiles, and extreme values of the population median: 
```{r, echo=FALSE}

apply(B,1,median) %>% summary() %>% t() %>%  pandoc.table()

```

We can also visualize the distribution of the bootstrapped median:
```{r, echo=FALSE, fig.width=4,fig.height=2.5}
ggplot(data.frame(median=apply(B,1,median)),aes(factor(median)))+
    geom_histogram(binwidth=1, color='grey', fill='powderblue')+
    labs(title="Distribution of Median from 10 Bootrap Samples",
         x='Median')+
    theme(plot.title = element_text(size = 11, face='bold'))

```


Of course, __R=10__ random samples are not enough to generate any meaningful distributions. We can repeat the same bootstrap exercise with __R=1000__ random samples, which gives us a matrix B sized 1000 by 9.
```{r}

R = 1000  # number of bootstrap samples
n = 9     # sample size

# set up a empty Rxn matrix B
B = matrix(nrow = R, ncol = n,
           dimnames = list(1:R, LETTERS[1:n]))
# loop R times
set.seed(111)
for(i in 1:R){
    # draw random samples from x
    B[i,] = sample(x, size = n, replace = TRUE)
}

```

Taking the row medians of bootstrap matrix __B__ gives us the empirical distribution of the median.

__Empirical Quantiles of Bootstrpped Median__
```{r, echo=FALSE, fig.width=4,fig.height=2.5}
# calculate row medians of B
boot_med = apply(B,1,median)

boot_med %>% 
    quantile(c(0, 0.01, 0.05, 0.10, 0.50, 0.90, 0.95, 0.99, 1)) %>% 
    t() %>%  pandoc.table()

ggplot(data.frame(median=boot_med), aes(factor(median)))+
    geom_histogram(binwidth=1, color='grey', fill='powderblue')+
    labs(title="Distribution of Median from 1000 Bootrap Samples",
         x='Median')+
    theme_classic()+
    theme(plot.title = element_text(size = 11, face='bold'))

```

### 90% Confidence Interval
Given the quantiles above, we can infer with 90% confidence that the median of the population __X__ is between __`r quantile(boot_med,.05)`__ and __`r quantile(boot_med,.95)`__ (5% and 95% quantiles). In other words, if we drew 100 samples from the same population, 90 samples would have the median within that range, assuming our original sample was representative of the population.


# Boostrap Applications in Model Validation
The basic technique illustrated above can be extended to a variety of use cases. 


## Stability of Regression Coefficients



### The Data
We use the dataset from R's 'ISLR' package, which contains simulated credit default data. It contains 10,000 observations with the following variables:

- __Default__: Yes/No
- __Student__: Yes/No
- __Balance__
- __Income__

```{r}
# load default data
library(ISLR)
data(Default)
data = Default %>% filter(balance>0)
```

```{r echo=FALSE}
# print sample of data
head(data) %>% 
    format(big.mark=",", digits=3, nsmall=1) %>% 
    kable(caption = "Sample of Default Data",
          align = 'r',
          format='html', table.attr='width=40%')
```

### The Model  
We fit a simple logistic regression with Default as the dependent variable, and Student, Balance, and Income as the predictors.
```{r}
logit = glm(default ~ balance + student + income, 
              data=data, family = binomial(link = "logit"))
```

```{r echo=FALSE}
logit %>% summary() %>% coef() %>% 
    kable(caption = "Logistic Regression: Probability of Default",
          format='html', digits=6, table.attr='width=75%')

cat("<BR>")
```

The p-values for model coefficients suggests that __Balance__ and __Student__ are highly significant predictors of __Default__. Higher balances are associated with higher incidence of default, and being a student lowers the probability of default. __Income__ appears to be insignificant. 

### Boostrap Model Coefficients

As part of model validation, the fit of the model and the significance of its coefficients are typically tested. These parametric tests rely on assumptions about the underlying distributions (normal, F, t, Chi-Square, etc.)

We can use bootstrapping to calculate any complex test statistic and infer confidence intervals without making any assumptions about the distribution of these statistics.

Next, we use bootstrap to test the stability of regression coefficients.

```{r}

R = 10                          # number of bootstrap samples
n = nrow(data)                  # sample size
k = length(coef(logit))      # number of coefficients
    
# set up a empty Rxn matrix B
B = matrix(nrow = R, ncol = k,
           dimnames = list(paste("Sample",1:R), 
                           names(coef(logit))))
# loop R times
set.seed(111)
for(i in 1:R){
    # sample default data with replacement
    boot.data = data[sample(x = 1:n, size = n, replace = TRUE), ]
    # fit the model
    boot.logit = glm(default ~ balance + student + income, 
              data=boot.data, family = binomial(link = "logit"))
    # record the coefficients
    B[i,] = coef(boot.logit)
}

```

As a result, we have a __R x n__ matrix of bootstrapped regression coefficients.

```{r echo=FALSE}

matrix(coef(logit), nrow = 1,
       dimnames = list("Full Sample", 
                       c("Constant","Balance","Student","Income"))) %>% 
    rbind(B) %>% 
    format(scientific=F, digits=1) %>% 
    pandoc.table(emphasize.strong.rows=1, 
                 caption='Bootstrapped Logistic Regression Model Coefficients')

```


### Using *boot* function in R

R's *boot* package provides a convenient function to perform bootstrapping for a variety of measures. Since generating a high number of random samples can be time-consuming for large datasets, the *boot* function allows to split the bootstrapping job across a cluster of computers or across multiple cores of the local CPU. 

```{r cache=TRUE}
library(boot)
library(parallel)

# function to return bootstrapped coefficients
myLogitCoef <- function(data, indices, formula) {
  d <- data[indices,]
  fit <- glm(formula, data=d, family = binomial(link = "logit"))
  return(coef(fit))
}

# set up cluster
cl<-makeCluster(4)
clusterExport(cl, 'myLogitCoef')

set.seed(373)
coef.boot <- boot(data=data, statistic=myLogitCoef, R=1000, 
                formula=default ~ balance + student + income,
                parallel = 'snow', ncpus=4, cl=cl)
stopCluster(cl)

```

### Univariate Distribution of Boostrapped Coefficients

The plots below illustrate the distribution of bootstrapped model coefficients and 95% empirical confidence intervals (thick black line at the bottom of the chart). For comparison, the graphs also show the normal distributions (orange bell curves) and 95% confidence intervals (thick orange lines) that are implied by the coefficients and their standard errors (assuming normality). 

```{r echo=FALSE, cache=TRUE,message=FALSE, fig.width=6, fig.height=3}

coef.comp <- 
  data.frame(
    var = row.names(coef(summary(logit))),
    coef = coef(summary(logit))[,1],
    se = coef(summary(logit))[,2])

ci <- confint(logit)
ci.boot <- confint(coef.boot, type='perc')


for(i in 2:nrow(coef.comp)){
  ci.df <- data.frame(x = range(ci[i,]),
                      x.bt = range(ci.boot[i,])) %>% 
    mutate(y.max = max(density(coef.boot$t[,i])$y),
           y = y.max/100, 
           y.bt = -y.max/100)
  
  p <- ggplot(data.frame(coeff = coef.boot$t[,i]), aes(x=coeff)) +
    geom_histogram(aes(y=..density..), 
                   binwidth = diff(range(coef.boot$t[,i]))/30,
                   fill='powderblue', color='grey') +
    stat_function(fun = dnorm, color='darkorange1',
                  args = list(mean = coef.comp$coef[i], 
                              sd = coef.comp$se[i])) + 
    geom_line(aes(x, y), ci.df, size=2, color='darkorange1') +
    geom_line(aes(x.bt, y.bt), ci.df, size=2) +
    ggtitle(coef.comp$var[i])
  print(p)
}
 

```

We can also examine the correlation between model coefficients by plotting their multivariate distributions. 

- Each dot represents a set of model coefficient estimates from each bootstrap iteration.
- The orange dashed lines mark 50%, 95%, and 99% confidence intervals.

```{r echo=FALSE, fig.width=6, fig.height=4}

B <- data.frame(coef.boot$t)
names(B) <- names(coef(logit))
ggplot(B, aes(x=studentYes, y=balance)) +
  geom_point(size=.7, color='navyblue')+
    stat_ellipse(type = "norm", linetype = 1, level = 0.001, color='darkorange1', size=2) +
    stat_ellipse(type = "norm", linetype = 2, level = 0.50, color='darkorange1', size=.8) +
  stat_ellipse(type = "norm", linetype = 2, level = 0.95, color='darkorange1', size=.8) +
  stat_ellipse(type = "norm", linetype = 2, level = 0.99, color='darkorange1', size=.8) +
  ggtitle('Bivariate Distribution of Bootstrapped Regression Coefficients')


```

### Conclusion

The plots above suggest that in the case of our data, the model coefficients distribution *approximates* normal. However, the bootstrapped confidence intervals have the advantage of not relying on any distributional assumptions. Moreover, they allow us to validate the appropriateness of using normality assumption.


## Model Prediction Accuracy


### Area under the ROC curve

The Receiver Operating Characteristic (ROC) curve for our default predicting logistic regression moddel is plotted below. The Area under the ROC curve (AUC) is one of widely-used measures of accuracy for classification models. 

- When AUC=1.00, the model assigns all observation to their true class with perfect accuracy. 
- When AUC=0.50, the ROC curve is equivalent to the 45-degree line. It indicates the model is as accurate as guessing at random.
- When AUC<0.50, the model accuracy is *worse* than guessing at random.

```{r echo=F, fig.width=4, fig.height=4}
library(ROCR)

#score test data set
prob = predict(logit, type='response', data)
pred = prediction(prob, data$default)

# auc
auc = performance(pred,"auc")@y.values[[1]][1]

perf <- performance(pred,"tpr","fpr")
plot(perf, main= paste("Logistic Regression ROC Curve: AUC =", round(auc,2)), cex.main=1)
abline(a=0, b = 1, col='darkorange1')


```

### Bootstrap estimates of prediction error
```{r}

R = 10
n = nrow(data)

# empty Rx2 matrix for bootstrap results 
B = matrix(nrow = R, ncol = 2,
           dimnames = list(paste('Sample',1:R),
                           c("auc_orig","auc_boot")))

set.seed(701)
for(i in 1:R){
    
    obs.boot <- sample(x = 1:n, size = n, replace = T)
    data.boot <- data[obs.boot, ]
    logit.boot <- glm(default ~ income + student , 
                      data=data.boot,
                      family = binomial(link = "logit"))
    
    # apply model to original data
    prob = predict(logit.boot, type='response', data)
    pred = prediction(prob, data$default)
    auc = performance(pred,"auc")@y.values[[1]][1]
#     cmat = table(predict(logit.boot, type='response', data)>0.30,
#                data$default=='Yes')
#     sens = cmat[2,2]/sum(cmat[,2])
    B[i, 1] = auc
    
    # apply model to bootstrap data
    prob = predict(logit.boot, type='response', data.boot)
    pred = prediction(prob, data.boot$default)
    auc = performance(pred,"auc")@y.values[[1]][1]
#     cmat = table(predict(logit.boot, type='response', data.boot)>0.30,
#                data.boot$default=='Yes')
#     sens = cmat[2,2]/sum(cmat[,2])
    B[i, 2] = auc
}

B = as.data.frame(B) %>% 
    mutate(optim = auc_boot - auc_orig)

colMeans(B) %>% format(digits=6, scientific=F)

```

