---
title: "Bootstrapping Approach to Managing Model Uncertainty"
author: "Vadim Bondarenko<BR>FI Consulting"
date: "July 15, 2015"
output: 
  html_document: 
    highlight: haddock
    number_sections: yes
    theme: flatly
    toc: yes
---

<style>
caption{color:black;font-size=16px;font-weight: bold;}
</style>


```{r setup, include=FALSE}

library(knitr)
library(pander)
library(ggplot2)
library(ggthemes)
library(scales)
library(png)
library(grid)
library(caret)
library(car)
library(boot)
library(ROCR)
library(dplyr)

# options("scipen"=2)
opts_chunk$set(echo = TRUE, results='asis', fig.align = 'center')
theme_set(theme_classic())
panderOptions('table.split.table', Inf)
panderOptions("big.mark", ",")

```


```{r logo, echo=FALSE, fig.width=600/72*1.2, fig.height=147/72*1.2}
# logo
img <- readPNG('img/webinar_logo.png')
grid.raster(img)
```


# Introduction

Model validation is an important step in the modeling process and helps in assessing the reliability of models before they can be used in decision making. Model validity can be judged by the stability and reasonableness of the regression coefficients, the plausibility and usability of the regression function and ability to generalize inference drawn from the regression analysis. We introduce the basic concept of bootstrap and review its uses in regression model validation.


## When to use Bootstrap?

The bootstrap is a simulation-based procedure for estimating and validating distributions. Generally, it is useful when:

1. The theoretical distribution of a statistic is complicated or unknown.
2. The sample size is insufficient for straightforward statistical inference.

To illustrate, we use bootstrap to derive an empirical distribution and confidence intervals for a sample median, which lacks a theoretical distribution.

## Basic Example: Confidence Intervals of a Sample Median  

### The Data
Suppose we have a sample __x__ of size __n=9__, which is simply numbers 1 through 9. We assume that __x__ is a representative sample of independent observation from a larger unknown population __X__.

```{r}
# hypothetical sample x = {1,2, ... ,8,9}
x = seq(1:9)

# print x
matrix(x, nrow = 1, dimnames = list("x", paste('Obs', 1:9))) %>% 
    pandoc.table("Sample Observations x")

# print summary of x
summary(x) %>% t() %>%  pandoc.table("Summary of sample x")
```


The observe __median = `r median(x)`__ (the middle __Observation 5__), but if __x__ is a representative sample from a population __X__ and __`r median(x)`__ is the estimate of median for that population, what is the __90% confidence interval__ around that estimate?

There is no analytical way to derive the distribution of the median. Plus, the sample is too small. However, we can use bootstrap to derive it empirically from the data.

### Basic Bootstrap  
To illustrate the basic setup, we begin by randomly drawing **R=10** samples __with replacement__ from out  sample __x__. Each random draw is of the same length (n=9) as our original sample.

```{r}

# set up a empty matrix to populate Bootstrap draws
B = matrix(nrow = 10, ncol = 9,
           dimnames = list(paste('Bootstrap Sample',1:10), 
                           LETTERS[1:9]))
# loop 10 times
set.seed(111)
for(i in 1:10){
    # draw random samples from x
    B[i,] = sample(x, size = length(x), replace = TRUE) %>% sort()
}


```

As a result, we have a __R x n__ matrix of bootstrapped observations.  
```{r echo=FALSE}

matrix(x, nrow = 1,ncol = 9,
       dimnames = list("Original Sample x", 
                       paste("Obs", 1:9))) %>% 
    rbind(B) %>% 
    pandoc.table(emphasize.strong.cols=5,
                 emphasize.strong.rows=1, caption='Bootstraped Samples')

```


__Note:__ Since we drew 10 random samples of the same size as our original sample (9), any given observation **x<sub>i</sub>** will appear more than once in some samples and none at all in others. For example, the number 4 appears four times in Sample 1, but is missing completely from bootstrap Sample 2.


### Bootstrapping the Statistic of Interest

The middle column with __Observation 5__ contains the medians of each bootstrapped sample. It provides us with the empirical distribution of the median, from which we can infer the mean, quantiles, and extreme values of the population median: 
```{r, echo=FALSE}

apply(B,1,median) %>% summary() %>% t() %>%  pandoc.table()

```

We can also visualize the distribution of the bootstrapped median:
```{r, echo=FALSE, fig.width=4,fig.height=2.5}
ggplot(data.frame(median=apply(B,1,median)),aes(factor(median)))+
    geom_histogram(binwidth=1, color='grey', fill='powderblue')+
    labs(title="Distribution of Median from 10 Bootstrap Samples",
         x='Median')+
    theme(plot.title = element_text(size = 11, face='bold'))

```


Of course, __R=10__ random samples are not enough to generate any meaningful distributions. We can repeat the same bootstrap exercise with __R=1000__ random samples, which gives us a matrix B sized 1000 by 9.
```{r}

R = 1000  # number of bootstrap samples
n = 9     # sample size

# set up a empty Rxn matrix B
B = matrix(nrow = R, ncol = n,
           dimnames = list(1:R, LETTERS[1:n]))
# loop R times
set.seed(111)
for(i in 1:R){
    # draw random samples from x
    B[i,] = sample(x, size = n, replace = TRUE)
}

```

Drawing a sufficiently large amounts of bootstrap samples and taking the row medians of bootstrap matrix __B__ results in a nice bell-shaped distribution of our median estimate.

__Empirical Quantiles of Bootstrpped Median__
```{r, echo=FALSE, fig.width=4,fig.height=2.5}
# calculate row medians of B
boot_med = apply(B,1,median)

boot_med %>% 
    quantile(c(0, 0.01, 0.05, 0.10, 0.50, 0.90, 0.95, 0.99, 1)) %>% 
    t() %>%  pandoc.table()

ggplot(data.frame(median=boot_med), aes(factor(median)))+
    geom_histogram(binwidth=1, color='grey', fill='powderblue')+
    labs(title="Distribution of Median from 1000 Bootstrap Samples",
         x='Median')+
    theme_classic()+
    theme(plot.title = element_text(size = 11, face='bold'))

```

### Bootstrap Confidence Interval
Given the quantiles above, we can infer with __90% confidence__ that the median of the population __X__ is between __`r quantile(boot_med,.05)`__ and __`r quantile(boot_med,.95)`__ (5% and 95% quantiles). In other words, if we drew 100 samples from the same population, 90 samples would have the median within that range, assuming our original sample was representative of the population.


## Key Points

* Estimator of interest
    - can be an algorithm of almost any complexity
    - bootstrapped estimate must be smooth function of data to be useful
* The original sample must be a fair representation of the underlying population
    - often a heroic assumption
* Simulation replaces theoretical calculation
    - removes need for math skills
    - does not remove need for thought, domain knowledge, and creativity
* Mitigate risk
    - validate code *very* carefully - garbage in, garbage out
    - insure the original sample is representative and observations are independent and identically distributed
    - increase number of random samples (large enough or as much as computationally feasible)



# Bootstrap Applications in Model Validation
The basic technique illustrated above can be extended to a variety of use cases, including:

1. Goodness of fit (F-stat, R<sup>2</sup>, AIC)
2. Statistical significance of the estimated parameters (t-test and p-values)
3. Residuals diagnostics (normality, heteroscedastisity)
4. Prediction Accuracy (Mean Squared Errors, Accuracy, Sensitivity/Specificity, Kolmogorov-Smirnov stat, etc.)
5. Estimates of model bias & variance (similar to cross-validation)

We can use bootstrapping to examine the distribution of these characteristics and their sensitivity to the natural variation in the sample without the need for collecting additional data.

Below, we provide just a few examples of its application, but the possibilities of using bootstrap methodology for model validation are limitless.



## Stability of Regression Coefficients

Once a regression model is estimated, the statistical significance of coefficients is tested with parametric tests. In this section we supplement traditional tests by examining the distribution of coefficient estimates through bootstrapping.


### The Data

We use a dataset consisting of 1,000 consumer credit profiles obtained from a German bank. For each consumer the binary response variable "credit" (Good/Bad) is available. In addition, 20 covariates that are assumed to influence creditworthiness were recorded. Examples of included predictors are listed below.

- Account Balance
- Payment Status
- Savings/Stocks Value
- Length of Current Employment
- Installment % of Income
- Marital Status / Gender
- Occupation
- Duration at Current Address
- Rent/Own

The full dataset description is available [here](https://onlinecourses.science.psu.edu/stat857/node/222).

```{r}
# load credit data
library(caret)
data(GermanCredit)
data = GermanCredit %>% rename(credit = Class)
```

### The Model  
We fit a simple logistic regression to predict the probability of **Good vs. Bad** credit quality.

```{r cache=TRUE}
set.seed(304)
logit = glm(credit ~
                 Amount + Age + Duration +
                 # Purpose.NewCar+
                Personal.Male.Single+
                 Purpose.UsedCar+
                 Property.RealEstate
            , 
            data=data, family = binomial(link = "logit"))

```

```{r echo=FALSE}
logit %>% summary() %>% coef() %>% 
    kable(caption = "Logistic Regression: Probability of 'Good' Credit Profile",
          format='html', digits=4, table.attr='width=75%')

cat("<BR>")
```

The p-values for model coefficients suggests that __Balance__ and __Student__ are highly significant predictors of __Default__. Higher balances are associated with higher incidence of default, and being a student lowers the probability of default. __Income__ appears to be insignificant. 

### Bootstrap Model Coefficients

As part of model validation, the fit of the model and the significance of its coefficients are typically tested. These parametric tests rely on assumptions about the underlying distributions (normal, F, t, Chi-Square, etc.)

We can use bootstrapping to calculate any complex test statistic and infer confidence intervals without making any assumptions about the distribution of these statistics.

Next, we use bootstrap to test the stability of regression coefficients.

```{r}

R = 10                          # number of bootstrap samples
n = nrow(data)                  # sample size
k = length(coef(logit))      # number of coefficients
    
# set up a empty Rxn matrix B
B = matrix(nrow = R, ncol = k,
           dimnames = list(paste("Sample",1:R), 
                           names(coef(logit))))
# loop R times
set.seed(111)
for(i in 1:R){
    # sample default data with replacement
    boot.data = data[sample(x = 1:n, size = n, replace = TRUE), ]
    # fit the model
    boot.logit = glm(logit$formula, 
              data=boot.data, family = binomial(link = "logit"))
    # record the coefficients
    B[i,] = coef(boot.logit)
}

```

As a result, we have a __R x k__ matrix of bootstrapped regression coefficients.

```{r echo=FALSE}

matrix(coef(logit), nrow = 1,
       dimnames = list("Full Sample", 
                       gsub("\\."," ", names(coef(logit)))
                       )
       ) %>% 
    rbind(B) %>% 
    format(scientific=F, digits=1) %>% 
    pandoc.table(emphasize.strong.rows=1, 
                 caption='Bootstrapped Logistic Regression Model Coefficients')

```


### Using *boot* function in R

R's *boot* package provides a convenient function to perform bootstrapping for a variety of measures. Since generating a high number of random samples can be time-consuming for large datasets, the *boot* function allows to split the bootstrapping job across a cluster of computers or across multiple cores of the local CPU. 

```{r cache=TRUE}
library(boot)
library(parallel)

# function to return bootstrapped coefficients
myLogitCoef <- function(data, indices, formula) {
  d <- data[indices,]
  fit <- glm(formula, data=d, family = binomial(link = "logit"))
  return(coef(fit))
}

# set up cluster
cl<-makeCluster(4)
clusterExport(cl, 'myLogitCoef')

set.seed(373)
coef.boot <- boot(data=data, statistic=myLogitCoef, R=1000, 
                formula= logit$formula,
                parallel = 'snow', ncpus=4, cl=cl)
stopCluster(cl)

```

### Univariate Distribution of Bootstrapped Coefficients

The plots below illustrate the distribution of bootstrapped model coefficients and 95% empirical confidence intervals (thick black line at the bottom of the chart). For comparison, the graphs also show the normal distributions (orange bell curves) and 95% confidence intervals (thick orange lines) that are implied by the estimated coefficients and their standard errors (assuming normality). 

```{r echo=FALSE, cache=TRUE,message=FALSE, fig.width=6, fig.height=3}

coef.comp <- 
  data.frame(
    var = row.names(coef(summary(logit))),
    coef = coef(summary(logit))[,1],
    se = coef(summary(logit))[,2])

ci <- confint(logit)
ci.boot <- confint(coef.boot, type='perc')


for(i in 2:nrow(coef.comp)){
  ci.df <- data.frame(x = range(ci[i,]),
                      x.bt = range(ci.boot[i,])) %>% 
    mutate(y.max = max(density(coef.boot$t[,i])$y),
           y = y.max/100, 
           y.bt = -y.max/100)
  
  p <- ggplot(data.frame(coeff = coef.boot$t[,i]), aes(x=coeff)) +
    geom_histogram(aes(y=..density..), 
                   binwidth = diff(range(coef.boot$t[,i]))/30,
                   fill='powderblue', color='grey') +
    stat_function(fun = dnorm, color='darkorange1', size=.9,
                  args = list(mean = coef.comp$coef[i], 
                              sd = coef.comp$se[i])) + 
    geom_line(aes(x, y), ci.df, size=2, color='darkorange1') +
    geom_line(aes(x.bt, y.bt), ci.df, size=2) +
    ggtitle(coef.comp$var[i])
  print(p)
}
 

```

We can also examine the correlation between model coefficients by plotting their multivariate distributions. 

- Each dot represents a set of model coefficient estimates from each bootstrap iteration.
- The orange dashed lines mark 50%, 95%, and 99% confidence intervals.

```{r echo=FALSE, fig.width=6, fig.height=4}

B <- data.frame(coef.boot$t)
names(B) <- names(coef(logit))
ggplot(B, aes(x=Duration, y=Amount)) +
  geom_point(size=.7, color='navyblue')+
    stat_ellipse(type = "norm", linetype = 1, level = 0.001, color='darkorange1', size=2) +
    stat_ellipse(type = "norm", linetype = 2, level = 0.50, color='darkorange1', size=.8) +
  stat_ellipse(type = "norm", linetype = 2, level = 0.95, color='darkorange1', size=.8) +
  stat_ellipse(type = "norm", linetype = 2, level = 0.99, color='darkorange1', size=.8) +
  ggtitle('Bivariate Distribution of Bootstrapped Regression Coefficients')


```

### Conclusion

Model assumptions are important for the quality of predictions from a regression model. Better predictions will result from a model that satisfies its underlying assumptions. However, assumptions can never be fully met in empirical data.

The plots above suggest that in the case of our data, the model coefficients distribution *approximates* normal. The bootstrapped confidence intervals can provide a sanity check for relying on the distributional assumptions inherent to parametric tests. 

The same methodology can be used to bootstrap the distribution of any other complex regression model tests statistics and diagnostics (i.e. R<sup>2</sup>, MSE, AIC, BIC, etc.)


## Validation of Model Prediction Accuracy

Bootstrap in the context of three types of validation:

1. __Apparent:__ 
    * Performance on same data used to train model (aka "in-sample" fit)
    * Easy to perform (often part of standard regression output)
    * Optimistic estimates of performance
    * Over-fitting - Model fine-tuned to training data. Testing on new data may show disappointing results.
    
2. __Internal:__ 
    * Performance on same population underlying the sample
    * Honest estimate of performance on observations similar to the training sample
    * Indicates upper limit to expected performance in other settings
    * Three common techniques:
        + Split Sample: Training / Testing
        + Cross-validation (repeated training/testing splits)
        + __Bootstrap__ (demonstrated below)
    
3. __External:__ 
    * Performance on related but slightly different population (aka "out-of-sample")
    * Sample from a nearby geography or from a following time period

In the following section, we demonstrate how to use bootstrap to perform internal validation of model prediction accuracy, as measured by Area under the ROC curve.


### Area under the ROC curve

The Receiver Operating Characteristic (ROC) curve for our default predicting logistic regression model is plotted below. The Area under the ROC curve (AUC) is a widely-used measure of accuracy for classification models. Its meaning can be interpreted as follows:

- When AUC=1.00, the model assigns all observation to their true class with perfect accuracy. 
- When AUC=0.50, the ROC curve is equivalent to the 45-degree line. It indicates the model is as accurate as guessing at random.
- When AUC<0.50, the model accuracy is *worse* than guessing at random.

We can derive the ROC curve and calculate AUC for the logistic regression fitted above using R's *ROCR* package.

```{r fig.width=4, fig.height=4}
library(ROCR)

# score the same training data set on which the model was fit
prob = predict.glm(logit, type='response', newdata =  data)
pred = prediction(prob, data$credit)

# AUC
auc = performance(pred,"auc")@y.values[[1]][1]

# ROC curve
perf <- performance(pred,"tpr","fpr")
plot(perf, col="navyblue", cex.main=1,
     main= paste("Logistic Regression ROC Curve: AUC =", round(auc,3)))
abline(a=0, b = 1, col='darkorange1')

```

The logit model's apparent, in-sample __AUC=`r round(auc,3)`__, which is unrealistically high, because the predictions where made for the same dataset on which the model was estimated. As mentioned before, to get a more realistic measure of the model's predictive accuracy, we need to apply internal (cross-validation, bootstrap) and/or external (new data, if available) validation techniques.

In the following section, we demonstrate the use of the bootstrap to adjust the apparent accuracy rate to get an idea of its hypothetical perform mace in predicting the outcomes for additional data sampled from a similar population, without actually collecting more data.



### Bootstrap Estimates of Prediction Accuracy

As a basic illustration, we generate **R=10** bootstrap samples, estimate the model on each, and then apply each fitted model to the *original sample* to give *R* estimates AUC. The overall estimate of prediction accuracy is the average of these *R* estimates (see first column in the table below). Further, we recalculate prediction accuracy when the fitted model is applied to the *bootstrap sample itself* (see the second column below).

```{r}

R = 10
n = nrow(data)

# empty Rx2 matrix for bootstrap results 
B = matrix(nrow = R, ncol = 2,
           dimnames = list(paste('Sample',1:R),
                           c("auc_orig","auc_boot")))

set.seed(701)
for(i in 1:R){
    
    # draw a random sample
    obs.boot <- sample(x = 1:n, size = n, replace = T)
    data.boot <- data[obs.boot, ]
    
    # fit the model on bootstrap sample
    logit.boot <- glm(logit$formula , 
                      data=data.boot,
                      family = binomial(link = "logit"))
    
    # apply model to original data
    prob1 = predict(logit.boot, type='response', data)
    pred1 = prediction(prob1, data$credit)
    auc1 = performance(pred1,"auc")@y.values[[1]][1]
    B[i, 1] = auc1
    
    # apply model to bootstrap data
    prob2 = predict(logit.boot, type='response', data.boot)
    pred2 = prediction(prob2, data.boot$credit)
    auc2 = performance(pred2,"auc")@y.values[[1]][1]
    B[i, 2] = auc2
}

```



```{r echo=FALSE}
df = as.data.frame(B) %>% 
    rbind(colMeans(B)) %>% 
    mutate(optim = auc_boot - auc_orig) %>% 
    round(digits = 3)

row.names(df) <- c(paste("Bootstrap Sample", 1:R), 'AVERAGE')
names(df) <- c("AUC on original sample",
               "AUC on bootstrap sample",
               "Optimism")


pandoc.table(
    df, emphasize.rownames=F,
    emphasize.strong.rows=R+1
)


```

Not surprisingly, the values in the second column are higher on the average than those in the first column. The improved bootstrap AUC estimate focuses on the difference between the first and second columns, called appropriately the "optimism"; it is the amount by which the average AUC (or " the apparent prediction accuracy") overestimates the true prediction accuracy. The overall estimate of optimism is the average of the *R* differences between the first and second columns, a value of __`r df[R+1,3]`__ in this example.

Once an estimate of optimism is obtained, it is subtracted from the apparent AUC to obtain an improved estimate of prediction accuracy. Here we obtain __`r round(auc,3)` - `r df[R+1,3]` = `r round(auc,3) - df[R+1,3]`__. 

```{r echo=FALSE, cache=TRUE}

R = 200

# empty Rx2 matrix for bootstrap results 
B2 = matrix(nrow = R, ncol = 2,
           dimnames = list(paste('Sample',1:R),
                           c("auc_orig","auc_boot")))

set.seed(602)
for(i in 1:R){
    
    # draw a random sample
    obs.boot <- sample(x = 1:n, size = n, replace = T)
    data.boot <- data[obs.boot, ]
    
    # fit the model on bootstrap sample
    logit.boot <- glm(logit$formula , 
                      data=data.boot,
                      family = binomial(link = "logit"))
    
    # apply model to original data
    prob1 = predict(logit.boot, type='response', data)
    pred1 = prediction(prob1, data$credit)
    auc1 = performance(pred1,"auc")@y.values[[1]][1]
    B2[i, 1] = auc1
    
    # apply model to bootstrap data
    prob2 = predict(logit.boot, type='response', data.boot)
    pred2 = prediction(prob2, data.boot$credit)
    auc2 = performance(pred2,"auc")@y.values[[1]][1]
    B2[i, 2] = auc2
}

B2df <- as.data.frame(B2)

optim = round(mean(B2df$auc_boot) - mean(B2df$auc_orig),3)

auc.adj = round(auc,3) - optim

```



Of course 10 bootstrap samples are too few; repeating with `r R` samples gave a value of  for the simple bootstrap estimate, and an estimate of `r optim` for the optimism leading to the value __`r round(auc,3)` - `r optim` = `r auc.adj`__ for the improved estimate of prediction accuracy. Essentially, we have added a *bias correction* to the apparent AUC.

To summarize:  
```{r echo=FALSE}

cbind(c("Apparent 'in-sample' AUC:", "Optimism", "Bootstrapped AUC:"),
      c(round(auc,3), optim, auc.adj)) %>% 
    kable(caption = "Logistic Regression: Internally-Validated Prediction Accuracy",
          format='html', digits=3, table.attr='width=35%')
cat("<BR>")
```



**For more details on this approach see:**  
*Efron, B., & Tibshirani, R. (1993). "An Introduction to the Bootstrap". Chapman and Hall*
